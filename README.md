# bayes-modle
# 朴素贝叶斯分类
## 目的
使用概率分布进行分类、学习朴素贝叶斯分类器进行文本分类以及附加题：过滤垃圾邮件。
## 原理
本次实验中，我首先构建了一个Bayes_classifier类作为Bayes模型。这个模型针对输入的数据集产生与数据集对应的类概率，特征在类下的概率。并以此为基础对新输入的数据进行正反类在输入特征下概率的计算，比较大小后输出大概率事件。  
## 流程
[![bGiRMD.png](https://s4.ax1x.com/2022/03/02/bGiRMD.png)](https://imgtu.com/i/bGiRMD)  
### 1. 建立Bayes_classifier类
Bayes_classifier类的目的就是根据用户所给的数据集算出数据本身的类概率和每个特征在类下的条件概率，只要这两个算出来，当有新的数据进来时，我们就可以根据数据包含的特征算出他在两个类下的条件概率，分别乘以类概率就能算出贝叶斯公示的分子，因为分母是特征概率，两个假设类的特征概率都是一样的，我们最终的目标是比较两个的大小，所以只比较分子的大小就可以。  
LoadAndInit（）计算类概率，P_feature_category（）计算特征在类下的概率。
### 2.文档分类
文档分类这个问题很简单，我们的特征定义的是每个词，即每个词对应一个向量。我们将数据传入bayes_classifier类，这时我们的bayes_classifier类就能计算出这个数据集在我们定义的特征下的两个所需概率。之后我们有两个测试数据，在本实验中我将这两个数据定义在了一个列表testdata之中，将这个列表的元素逐个传入模型之后，模型对正负类的概率进行大小对比，然后输出概率大的类。  
### 3.垃圾邮件分类
垃圾邮件在spam中，正常邮件在ham中，我们的主要操作就是从这所有的文件中得到数据保存到列表中，然后传入我们的bayes_classifier就行。   
我们这次获得数据的最大阻碍就是分词，我这里利用了O(n)的遍历方法遍历每一个字符，只有它是字母和数字时才能进入列表，之间碰到非字母和字符时加一个空格，最后以空格分词就能得到我们想要的数据。  
## 结果
对数据集在不同数量情况下的预测效果，最终的错误率随训练集数据量变化如下：  
[![bGFnF1.png](https://s4.ax1x.com/2022/03/02/bGFnF1.png)](https://imgtu.com/i/bGFnF1)
## 结论
本次实验，首先从实验结果来看，效果是非常不错的，文档分类和垃圾邮件识别这两个任务通过朴素贝叶斯模型都能得到很好的预测，这说明了贝叶斯公示在机器学习领域巨大的价值。  
此外，可以发现，朴素贝叶斯在垃圾邮件识别上的收敛是比较快的，当训练集和测试集达到一比一时就能有一个非常好的预测效果。  
但是值得注意的一点是，本次实验中，朴素贝叶斯的特征处理是每个单词作为一个特征。这会造成我们的特征向量会随着数据的增加而迅速增加，而我们使用到的训练特征矩阵会变得非常大，这可能会使得训练成为一件不是那么容易的事情。另一方面，邮件的识别往往也不是仅仅通过单词就能识别，对句子整体的考虑可能会更重要，这也是本次实验的不足所在。而如果建立关联的单词，或者句子作为特征，又会导致特征没有直接选择单词那么容易实现，这也正是矛盾所在。所以，本次选择独立单词作为特征，有利也有弊。
